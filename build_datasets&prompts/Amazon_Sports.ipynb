{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7102dc31-e414-424d-9544-5067bdbbadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def get_df(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d95bd5-21e5-455c-a25b-87aba0eacc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'Sports_and_Outdoors'\n",
    "RAW_PATH = os.path.join('./data/', DATASET)\n",
    "DATA_FILE = 'reviews_{}_5.json.gz'.format(DATASET)\n",
    "META_FILE = 'meta_{}.json.gz'.format(DATASET)\n",
    "\n",
    "# download data if not exists\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    subprocess.call('mkdir ' + RAW_PATH, shell=True)\n",
    "if not os.path.exists(os.path.join(RAW_PATH, DATA_FILE)):\n",
    "    print('Downloading interaction data into ' + RAW_PATH)\n",
    "    subprocess.call(\n",
    "        'cd {} && curl -O http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_{}_5.json.gz'\n",
    "        .format(RAW_PATH, DATASET), shell=True)\n",
    "if not os.path.exists(os.path.join(RAW_PATH, META_FILE)):\n",
    "    print('Downloading item metadata into ' + RAW_PATH)\n",
    "    subprocess.call(\n",
    "        'cd {} && curl -O http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_{}.json.gz'\n",
    "        .format(RAW_PATH, DATASET), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64313936-6fe8-4710-92c1-f00e97f068c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_df(os.path.join(RAW_PATH, DATA_FILE))\n",
    "meta_df = get_df(os.path.join(RAW_PATH, META_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63999e89-c755-4468-9705-ce1be0f50a1f",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480f75c0-7905-4d7a-a6b6-bf0f3718dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Users: 35598\n",
      "# Items: 18357\n",
      "# Interactions: 296337\n",
      "Time Span: 2002-03-07/2014-07-23\n",
      "========after filtering=============\n",
      "# Users: 35598\n",
      "# Items: 18357\n",
      "# Interactions: 296337\n",
      "Time Span: 2002-03-07/2014-07-23\n"
     ]
    }
   ],
   "source": [
    "def filter_df(df):\n",
    "    while True:\n",
    "        asin_counts = df['asin'].value_counts()\n",
    "        reviewerName_counts = df['reviewerID'].value_counts()\n",
    "       \n",
    "        asin_to_remove = asin_counts[asin_counts < 5].index\n",
    "        reviewerName_to_remove = reviewerName_counts[reviewerName_counts < 5].index\n",
    "       \n",
    "        if len(asin_to_remove) == 0 and len(reviewerName_to_remove) == 0:\n",
    "            break\n",
    "\n",
    "        df = df[~df['asin'].isin(asin_to_remove)]\n",
    "        df = df[~df['reviewerID'].isin(reviewerName_to_remove)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "n_users = data_df['reviewerID'].value_counts().size\n",
    "n_items = data_df['asin'].value_counts().size\n",
    "n_clicks = len(data_df)\n",
    "min_time = data_df['unixReviewTime'].min()\n",
    "max_time = data_df['unixReviewTime'].max()\n",
    "time_format = '%Y-%m-%d'\n",
    "\n",
    "print('# Users:', n_users)\n",
    "print('# Items:', n_items)\n",
    "print('# Interactions:', n_clicks)\n",
    "print('Time Span: {}/{}'.format(\n",
    "    datetime.utcfromtimestamp(min_time).strftime(time_format),\n",
    "    datetime.utcfromtimestamp(max_time).strftime(time_format))\n",
    ")\n",
    "\n",
    "data_df = filter_df(data_df)\n",
    "\n",
    "print(\"========after filtering=============\")\n",
    "n_users = data_df['reviewerID'].value_counts().size\n",
    "n_items = data_df['asin'].value_counts().size\n",
    "n_clicks = len(data_df)\n",
    "min_time = data_df['unixReviewTime'].min()\n",
    "max_time = data_df['unixReviewTime'].max()\n",
    "time_format = '%Y-%m-%d'\n",
    "\n",
    "print('# Users:', n_users)\n",
    "print('# Items:', n_items)\n",
    "print('# Interactions:', n_clicks)\n",
    "print('Time Span: {}/{}'.format(\n",
    "    datetime.utcfromtimestamp(min_time).strftime(time_format),\n",
    "    datetime.utcfromtimestamp(max_time).strftime(time_format))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09bbdc3e-e0a9-474c-9cbc-00f253ae62e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand          32.82%\n",
       "price          19.57%\n",
       "salesRank      12.54%\n",
       "description     9.46%\n",
       "title           0.49%\n",
       "imUrl           0.17%\n",
       "asin            0.00%\n",
       "related         0.00%\n",
       "categories      0.00%\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_meta_df = meta_df[meta_df['asin'].isin(data_df['asin'])].reset_index(drop=True)\n",
    "all_items = set(useful_meta_df['asin'].values.tolist())\n",
    "\n",
    "def related_filter(related_dict):\n",
    "    out_dict = dict()\n",
    "    if related_dict is not np.nan:\n",
    "        for r in related_dict:\n",
    "            out_dict[r] = list(all_items & set(related_dict[r]))\n",
    "    return out_dict\n",
    "\n",
    "useful_meta_df['related'] = useful_meta_df['related'].apply(related_filter)\n",
    "\n",
    "df = useful_meta_df\n",
    "((df.isnull().sum())/df.shape[0]).sort_values(ascending=False).map(lambda x:\"{:.2%}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100aa783-3b68-43bb-84fe-d5b93bc20990",
   "metadata": {},
   "source": [
    "# build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bceb0b2-2f99-4c76-97d6-e6e22e1c310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = data_df.rename(columns={'asin': 'item_id', 'reviewerID': 'user_id', 'unixReviewTime': 'time'})\n",
    "out_df = out_df[['user_id', 'item_id', 'time']]\n",
    "out_df = out_df.drop_duplicates(['user_id', 'item_id', 'time'])\n",
    "out_df = out_df.sort_values(by=['user_id', 'time'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "\n",
    "uids = out_df['user_id'].unique()\n",
    "user2id = dict(zip(uids, range(1, len(uids) + 1)))\n",
    "# iids = sorted(out_df['item_id'].unique())\n",
    "iids = out_df['item_id'].unique()\n",
    "item2id = dict(zip(iids, range(1, len(iids) + 1)))\n",
    "\n",
    "out_df['user_id'] = out_df['user_id'].apply(lambda x: user2id[x])\n",
    "out_df['item_id'] = out_df['item_id'].apply(lambda x: item2id[x])\n",
    "\n",
    "\n",
    "useful_meta_df['item_id'] = useful_meta_df['asin'].apply(lambda x: item2id[x])\n",
    "# save data\n",
    "out_df.to_csv(RAW_PATH+'/inter.csv', index=False)\n",
    "useful_meta_df.to_csv(RAW_PATH+'/meta.csv', index=False)\n",
    "\n",
    "\n",
    "inter = out_df.drop(columns=['time'])\n",
    "inter.columns = ['user_id:token', 'item_id:token']\n",
    "inter.to_csv(RAW_PATH+'/Amazon_'+DATASET+'.inter', sep='\\t', index=False)\n",
    "\n",
    "df_txt = out_df.drop(columns=['time'])\n",
    "df_txt.to_csv(RAW_PATH+'/'+ DATASET+'.txt', sep=' ', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03e984d0-b83f-4261-8dee-06f2e63e7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(RAW_PATH+'/item2id.json', 'w') as f:\n",
    "    json.dump(item2id, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611481c-7d22-4b32-bb2f-1bed05270b33",
   "metadata": {},
   "source": [
    "# user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b97b5b-baf5-48e3-b6fb-ce2318b21637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "['Magnesium Fire Starter']\n",
      "[1, 2]\n",
      "['Magnesium Fire Starter', 'Survivor HK-106320 Outdoor Fixed Blade Knife 7 Overall WITH FIRE STARTER']\n",
      "[1, 2, 3]\n",
      "['Magnesium Fire Starter', 'Survivor HK-106320 Outdoor Fixed Blade Knife 7 Overall WITH FIRE STARTER', 'Big Bohica Kukhri Machete']\n",
      "[1, 2, 3, 4]\n",
      "['Magnesium Fire Starter', 'Survivor HK-106320 Outdoor Fixed Blade Knife 7 Overall WITH FIRE STARTER', 'Big Bohica Kukhri Machete', 'Ultralight Backpacking Canister Camp Stove with Piezo Ignition 3.9oz']\n",
      "[6]\n",
      "['San Angelo Shooting Vise Bench Rest']\n",
      "[6, 1]\n",
      "['San Angelo Shooting Vise Bench Rest', 'Magnesium Fire Starter']\n",
      "[6, 1, 7]\n",
      "['San Angelo Shooting Vise Bench Rest', 'Magnesium Fire Starter', 'Duke 0965 Animal Body Trap Setting Tool']\n",
      "[6, 1, 7, 8]\n",
      "['San Angelo Shooting Vise Bench Rest', 'Magnesium Fire Starter', 'Duke 0965 Animal Body Trap Setting Tool', 'Truglo Home Defense Fiber Optic 12-20Ga Sight']\n",
      "[6, 1, 7, 8, 9]\n",
      "['San Angelo Shooting Vise Bench Rest', 'Magnesium Fire Starter', 'Duke 0965 Animal Body Trap Setting Tool', 'Truglo Home Defense Fiber Optic 12-20Ga Sight', 'Allen Company Rifle Belt Ammo Carrier Pouch']\n",
      "[6, 1, 7, 8, 9, 10]\n",
      "['San Angelo Shooting Vise Bench Rest', 'Magnesium Fire Starter', 'Duke 0965 Animal Body Trap Setting Tool', 'Truglo Home Defense Fiber Optic 12-20Ga Sight', 'Allen Company Rifle Belt Ammo Carrier Pouch', 'Shotgun Shell Belt']\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "inter = pd.read_csv(RAW_PATH+'/inter.csv')\n",
    "useful_meta_df = pd.read_csv(RAW_PATH+'/meta.csv')\n",
    "\n",
    "# seq slide augmentation\n",
    "\n",
    "def prepare_data_augmentation(df):\n",
    "    max_item_list_len = 20\n",
    "\n",
    "    last_uid = None\n",
    "    uid_list, item_list, target, item_list_length = [], [], [], []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        uid, item_id = row['user_id'], row['item_id']\n",
    "        if last_uid != uid:\n",
    "            last_uid = uid\n",
    "            seq = []\n",
    "        else:\n",
    "            if len(seq) > max_item_list_len:\n",
    "                seq = seq[1:]\n",
    "            uid_list.append(uid)\n",
    "            item_list.append(seq[:])\n",
    "            target.append(item_id)\n",
    "            item_list_length.append(len(seq))\n",
    "        seq.append(item_id)\n",
    "\n",
    "    return uid_list, item_list, target, item_list_length\n",
    " \n",
    "\n",
    "uid_list, item_list, target, item_list_length = prepare_data_augmentation(inter)\n",
    "\n",
    "\n",
    "import ast\n",
    "# useful_meta_df\n",
    "useful_meta_df['categories'] = useful_meta_df['categories'].apply(ast.literal_eval)\n",
    "useful_meta_df['categories'] = useful_meta_df['categories'].apply(lambda x: x[0][2:])\n",
    "useful_meta_df['title'] = useful_meta_df['title'].fillna('missing')\n",
    "# columns_to_drop = ['asin', 'description','imUrl', 'related', 'salesRank']\n",
    "columns_to_drop = ['asin', 'description','imUrl', 'related']\n",
    "useful_meta_df_dropped = useful_meta_df.drop(columns=columns_to_drop)\n",
    "meta_dict = useful_meta_df_dropped.set_index('item_id').T.to_dict()\n",
    "\n",
    "requests = {}\n",
    "for i in range(len(item_list)):\n",
    "    uid, items = uid_list[i], item_list[i]\n",
    "    # print([uid] + items)\n",
    "    key = \":\".join(map(str, [uid] + items))\n",
    "    title_list = []\n",
    "    for item in items:\n",
    "        if meta_dict[item]['title'] == \"missing\":\n",
    "            title_list.append(meta_dict[item]['categories'])\n",
    "        else:\n",
    "            title_list.append(meta_dict[item]['title'])\n",
    "    if i < 10:\n",
    "        print(items)\n",
    "        print(title_list)\n",
    "    formatted_titles = [f'<{title}>' for title in title_list]\n",
    "    formatted_string = '; '.join(formatted_titles)\n",
    "    requests[key] = formatted_string\n",
    "\n",
    "import pickle\n",
    "with open(RAW_PATH+'/user_prompt_input.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(requests, pickle_file)\n",
    "    \n",
    "# with open(RAW_PATH+\"/user_prompt_input.pkl\", 'rb') as pickle_file:\n",
    "#     question_dic = pickle.load(pickle_file)\n",
    "\n",
    "system_input = \"\"\"Assume you are a sports_and_outdoors products recommendation expert.\n",
    "You will be provided with a user's historical purchases of sports_and_outdoors products in chronological order, given in the following format:\n",
    "<The title of item1>; <The title of item2>; <The title of item3>;... \n",
    "Please summarize the user's specific preference when purchasing sports_and_outdoors products. Note that your response should be a coherent paragraph of no more than 100 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ec7e21-b368-4a7a-9160-a6b0b25ae43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume you are a sports_and_outdoors products recommendation expert.\n",
      "You will be provided with a user's historical purchases of sports_and_outdoors products in chronological order, given in the following format:\n",
      "<The title of item1>; <The title of item2>; <The title of item3>;... \n",
      "Please summarize the user's specific preference when purchasing sports_and_outdoors products. Note that your response should be a coherent paragraph of no more than 100 words.\n",
      "<Magpul PTS MBUS Front &amp; Rear Back-UP Sight Set Black>; <Hogue Handall Full Size Grip Sleeve>; <Froglube CLP 4 Oz. Tub of Paste Gun Cleaner Lubricant Protectant>; <Strike Industries 1911 Torx Grip Screws With Stainless Steel x 4>\n"
     ]
    }
   ],
   "source": [
    "# with open(RAW_PATH+\"/user_prompt_input.pkl\", 'rb') as pickle_file:\n",
    "#     question_dic = pickle.load(pickle_file)\n",
    "# system_input = \"\"\"Assume you are a sports_and_outdoors products recommendation expert.\n",
    "# You will be provided with a user's historical purchases of sports_and_outdoors products in chronological order, given in the following format:\n",
    "# <The title of item1>; <The title of item2>; <The title of item3>;... \n",
    "# Please summarize the user's specific preference when purchasing sports_and_outdoors products. Note that your response should be a coherent paragraph of no more than 100 words.\n",
    "# \"\"\"\n",
    "# print(system_input + question_dic[\"23968:11182:990:3169:9388\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0ee7c-c23c-4c2b-97fc-27434fd15312",
   "metadata": {},
   "source": [
    "# item prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e007fe-8f40-4fb9-ab76-ce80917f7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = pd.read_csv(RAW_PATH+'/inter.csv')\n",
    "useful_meta_df = pd.read_csv(RAW_PATH+'/meta.csv')\n",
    "\n",
    "useful_meta_df['brand'] = useful_meta_df['brand'].fillna('missing')\n",
    "useful_meta_df['salesRank'] = useful_meta_df['salesRank'].fillna('missing')\n",
    "useful_meta_df['title'] = useful_meta_df['title'].fillna('missing')\n",
    "useful_meta_df['description'] = useful_meta_df['description'].fillna('missing')\n",
    "useful_meta_df['price'] = useful_meta_df['price'].fillna('missing')\n",
    "import ast\n",
    "useful_meta_df['categories'] = useful_meta_df['categories'].apply(ast.literal_eval)\n",
    "useful_meta_df['categories'] = useful_meta_df['categories'].apply(lambda x: x[0][2:])\n",
    "columns_to_drop = ['asin', 'imUrl', 'related', 'salesRank']\n",
    "# columns_to_drop = ['asin', 'imUrl', 'related', ]\n",
    "useful_meta_df_dropped = useful_meta_df.drop(columns=columns_to_drop)\n",
    "meta_dict = useful_meta_df_dropped.set_index('item_id').T.to_dict()\n",
    "\n",
    "# seq slide augmentation\n",
    "\n",
    "def prepare_data_augmentation(df):\n",
    "    max_item_list_len = 20\n",
    "\n",
    "    last_uid = None\n",
    "    uid_list, item_list, target, item_list_length = [], [], [], []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        uid, item_id = row['user_id'], row['item_id']\n",
    "        if last_uid != uid:\n",
    "            last_uid = uid\n",
    "            seq = []\n",
    "        else:\n",
    "            if len(seq) > max_item_list_len:\n",
    "                seq = seq[1:]\n",
    "            uid_list.append(uid)\n",
    "            item_list.append(seq[:])\n",
    "            target.append(item_id)\n",
    "            item_list_length.append(len(seq))\n",
    "        seq.append(item_id)\n",
    "\n",
    "    return uid_list, item_list, target, item_list_length\n",
    " \n",
    "\n",
    "uid_list, item_list, target, item_list_length = prepare_data_augmentation(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f401a64c-0468-45a8-8240-533561627063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of filtered_list 225141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_list = [sublist[-11:] for sublist in item_list if (len(sublist) >= 2 and len(sublist) <= 21)]\n",
    "print(\"length of filtered_list\", len(filtered_list))\n",
    "\n",
    "last_values = set(sublist[-1] for sublist in filtered_list)\n",
    "\n",
    "end_dict = {value: [] for value in last_values}\n",
    "\n",
    "for sublist in filtered_list:\n",
    "    last_value = sublist[-1]\n",
    "    end_dict[last_value].append(sublist)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "for key in end_dict:\n",
    "    if len(end_dict[key]) > 5:\n",
    "        end_dict[key] = random.sample(end_dict[key], 5)\n",
    "\n",
    "end_dict_text = {}\n",
    "for key in end_dict:\n",
    "    end_dict_text[key] = []\n",
    "    for ls in end_dict[key]:\n",
    "        ls_text = [meta_dict[item]['title'] for item in ls]\n",
    "        ls_text = [val for val in ls_text if val != \"missing\"]\n",
    "        end_dict_text[key].append(ls_text)\n",
    "\n",
    "\n",
    "end_dict_text_formatted = {}\n",
    "for item_id in end_dict_text.keys():\n",
    "    same_target_seqs = ''\n",
    "    for seq in end_dict_text[item_id]:\n",
    "        print(seq)\n",
    "        seq = ' -> '.join(seq) \n",
    "        seq = '[' + seq + '] # '\n",
    "        seq += \" \\n \"\n",
    "        same_target_seqs += seq\n",
    "    end_dict_text_formatted[item_id] = same_target_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24a5d491-d303-4189-b594-f5029cad8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_template(item_dict):\n",
    "    text = (\n",
    "        f\"The product name is {item_dict['title']}; \"\n",
    "        f\"brand is {item_dict['brand']}; \"\n",
    "        f\"categories are {item_dict['categories']}; \"\n",
    "        f\"price is {item_dict['price']}; \"\n",
    "        f\"The detailed description of the product is {item_dict['description'][:500]}......\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "item_meta_formatted = {}\n",
    "for item_id in item2id.values():\n",
    "    item_meta_formatted[item_id] = item_template(meta_dict[item_id])\n",
    "    \n",
    "def complete_prompt_gen(item_info, history):\n",
    "    prompt = f\"\"\"\n",
    "    Assume you are a sports_and_outdoors products recommendation expert. Please help me analyze a specific sports_and_outdoors product. You will be provided with the following information:\n",
    "    1) The basic information of the product: {item_info};\n",
    "    2) The historical purchase information of users who have bought this product: {history}. Here, different sequences are separated by '#', and each sequence is in LIST format, representing a certain user's historical purchases. Items in each sequence are separated by '->', and the last item in all sequences is the specific product mentioned above.\n",
    "    \n",
    "    Requirements:\n",
    "    1) Please briefly describe the given sports_and_outdoors product.\n",
    "    2) Based on the provided sequences, please analyze what type of users would purchase this specific product. Please do not generally say sports enthusiasts or outdoor enthusiasts, as all users in this context have a need to purchase sports_and_outdoors products. Instead, please provide a more detailed granularity.\n",
    "    Please provide your answer in JSON format, following this structure:\n",
    "    {{\n",
    "    \"item summary\": \"A description of the item, no more than 80 words.\", \n",
    "    \"potential user analysis\": \"what type of users would purchase this item, no more than 50 words.\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "item_prompt = {}\n",
    "for item_id in item2id.values():\n",
    "    item_info = item_meta_formatted[item_id]\n",
    "    if item_id in end_dict_text_formatted.keys():\n",
    "        history = end_dict_text_formatted[item_id]\n",
    "    else:\n",
    "        history = ' [None] '\n",
    "    item_prompt[item_id] = complete_prompt_gen(item_info, history)\n",
    "\n",
    "import pickle\n",
    "with open(RAW_PATH+'/item_prompt_input.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(item_prompt, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
